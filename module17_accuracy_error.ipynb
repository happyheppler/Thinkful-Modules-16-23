{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "\n",
    "# Change spam values to booleans\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 5572 points: 604\n",
      "Accuracy is 89.16%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model\n",
    "print('Number of mislabeled points out of a total {} points: {}'.format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "\n",
    "print('Accuracy is {}%'.format(100 - round((target != y_pred).sum() / data.shape[0] * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4770,   55],\n",
       "       [ 549,  198]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use built-in confusion matric from sklearn\n",
    "# The columns are prediction, rows are actual\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn the majority of our error is coming from times where we failed to identify a spam message. 549 of our 604 errors are from failing to identify spam. So we need to get a little bit better at identifying spam messages.\n",
    "\n",
    "Let's assume our goal is to identify spam (rather than identify ham).\n",
    "\n",
    "A false positive is when we identify something as spam that is not. In this case we had 55 of these. This is sometimes also called a \"Type I Error\" or a \"false alarm\"\n",
    "\n",
    "A false negative is therefore when we mistakenly identify something as not spam when it is. We had 549 of these. This is also called a \"Type II Error\" or a \"miss\"\n",
    "\n",
    "Sensitivity is the percentage of positives correctly identified, in our case 198/747 or 27%. This shows how good we are at catching positives, or how sensitive our model is to identifying positives.\n",
    "\n",
    "Specificity is just the opposite, the percentage of negatives correctly identified, 4770/4825 or 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4770, 55], [549, 198]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build your confusion matrix without sklearn\n",
    "ticker = -1\n",
    "ham_ham = 0\n",
    "spam_spam = 0\n",
    "ham_spam = 0\n",
    "spam_ham = 0\n",
    "\n",
    "for email in target:\n",
    "    ticker += 1\n",
    "    if email == True:\n",
    "        if y_pred[ticker] == True:\n",
    "            spam_spam += 1\n",
    "        elif y_pred[ticker] == False:\n",
    "            spam_ham += 1\n",
    "    elif email == False:\n",
    "        if y_pred[ticker] == True:\n",
    "            ham_spam += 1\n",
    "        elif y_pred[ticker] == False:\n",
    "            ham_ham += 1\n",
    "        \n",
    "confusion_matrix = [[ham_ham, ham_spam],\n",
    "                   [spam_ham, spam_spam]]\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4770, 55], [549, 198]]\n",
      "Sensitivity: 26.5\n",
      "Specificity: 98.9\n"
     ]
    }
   ],
   "source": [
    "# Calculate sensitivity and specificity\n",
    "sensitivity = round(spam_spam / (spam_ham + spam_spam) * 100, 1)\n",
    "specificity = round(ham_ham / (ham_ham + ham_spam) * 100, 1)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print('Sensitivity: {}\\nSpecificity: {}'.format(sensitivity, specificity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
